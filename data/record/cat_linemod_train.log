train epoch 0 step 0 seg 0.69059110  ver 0.25422847  precision 0.02193035  recall 0.52476907 
train epoch 0 step 0 seg 0.63345397  ver 0.28390700  precision 0.01238039  recall 0.00058561 
train epoch 0 step 0 seg 0.75455987  ver 0.25360614  precision 0.01210957  recall 0.99628609 
train epoch 0 step 0 seg_loss 0.69074333  ver_loss 0.24282765  precision 0.01102162  recall 0.35876647  loss1 0.93107629  loss2 0.93357110  loss3 931161.37500000 
train epoch 0 step 0 seg_loss 0.74209148  ver_loss 0.25949681  precision 0.00763432  recall 0.88199234  loss1 1.09877408  loss2 1.00158834  loss3 1539478.12500000 
val epoch 0 step 0 seg_loss 0.61340457  ver_loss 0.25536409  precision 0.00016627  recall 0.00033760  loss1 0.85049886  loss2 0.86876875  loss3 0.00742515 
train epoch 1 step 45 seg_loss 0.61890817  ver_loss 0.25151426  precision 0.00017697  recall 0.00045592  loss1 0.85973227  loss2 0.87042242  loss3 28644.15625000 
val epoch 1 step 1 seg_loss 0.51926506  ver_loss 0.24613197  precision 1.00000000  recall 0.00033760  loss1 0.73640746  loss2 0.76539749  loss3 0.00334526 
train epoch 2 step 90 seg_loss 0.52272201  ver_loss 0.24315493  precision 1.00000000  recall 0.00050448  loss1 0.74244642  loss2 0.76587695  loss3 18998.87695312 
train epoch 2 step 90 seg_loss 0.52353805  ver_loss 0.25103986  precision 1.00000000  recall 0.00040163  loss1 0.75435710  loss2 0.77457792  loss3 22443.42187500 
val epoch 2 step 2 seg_loss 0.43524882  ver_loss 0.23635752  precision 1.00000000  recall 0.00033760  loss1 0.65059477  loss2 0.67160648  loss3 0.00335570 
train epoch 3 step 135 seg_loss 0.44668022  ver_loss 0.23598602  precision 1.00000000  recall 0.00051577  loss1 0.68059587  loss2 0.68266630  loss3 29231.81640625 
val epoch 3 step 3 seg_loss 0.34899041  ver_loss 0.22869691  precision 1.00000000  recall 0.00033760  loss1 0.56773013  loss2 0.57768744  loss3 0.00218299 
train epoch 4 step 180 seg_loss 0.38182390  ver_loss 0.22526751  precision 1.00000000  recall 0.00030568  loss1 0.60064256  loss2 0.60709137  loss3 30466.24609375 
val epoch 4 step 4 seg_loss 0.31971663  ver_loss 0.22147934  precision 1.00000000  recall 0.00033760  loss1 0.54559261  loss2 0.54119599  loss3 0.00218903 
train epoch 5 step 225 seg_loss 0.33448505  ver_loss 0.24520689  precision 1.00000000  recall 0.00044147  loss1 0.62043899  loss2 0.57969189  loss3 36947.27734375 
val epoch 5 step 5 seg_loss 0.29543269  ver_loss 0.21648705  precision 1.00000000  recall 0.00033760  loss1 0.51511222  loss2 0.51191968  loss3 0.00255469 
train epoch 6 step 270 seg_loss 0.27259171  ver_loss 0.23110950  precision 1.00000000  recall 0.00035106  loss1 0.52142346  loss2 0.50370121  loss3 17834.13281250 
val epoch 6 step 6 seg_loss 0.23669584  ver_loss 0.21221611  precision 1.00000000  recall 0.00033760  loss1 0.45812565  loss2 0.44891223  loss3 0.00196279 
train epoch 7 step 315 seg_loss 0.25442234  ver_loss 0.20581385  precision 1.00000000  recall 0.00037738  loss1 0.48177797  loss2 0.46023616  loss3 42889.04687500 
val epoch 7 step 7 seg_loss 0.20774575  ver_loss 0.20761016  precision 1.00000000  recall 0.00033760  loss1 0.43041292  loss2 0.41535589  loss3 0.00225982 
train epoch 8 step 360 seg_loss 0.22089057  ver_loss 0.21927066  precision 1.00000000  recall 0.00040334  loss1 0.47148648  loss2 0.44016123  loss3 14814.82031250 
val epoch 8 step 8 seg_loss 0.17625408  ver_loss 0.20536603  precision 1.00000000  recall 0.00033760  loss1 0.39918706  loss2 0.38162032  loss3 0.00174850 
train epoch 9 step 405 seg_loss 0.22300351  ver_loss 0.20462303  precision 1.00000000  recall 0.00037580  loss1 0.45871073  loss2 0.42762655  loss3 11345.83593750 
val epoch 9 step 9 seg_loss 0.15556750  ver_loss 0.20337448  precision 1.00000000  recall 0.00033760  loss1 0.38489181  loss2 0.35894200  loss3 0.00178777 
train epoch 10 step 450 seg_loss 0.16760099  ver_loss 0.19605744  precision 1.00000000  recall 0.00031974  loss1 0.40133792  loss2 0.36365843  loss3 24203.81640625 
val epoch 10 step 10 seg_loss 0.13964568  ver_loss 0.20193328  precision 1.00000000  recall 0.00033760  loss1 0.36461994  loss2 0.34157887  loss3 0.00141453 
train epoch 11 step 495 seg_loss 0.16794264  ver_loss 0.23542660  precision 1.00000000  recall 0.00039188  loss1 0.44384804  loss2 0.40336925  loss3 19199.14453125 
val epoch 11 step 11 seg_loss 0.10519812  ver_loss 0.20063311  precision 1.00000000  recall 0.00033760  loss1 0.33442882  loss2 0.30583128  loss3 0.00125449 
train epoch 12 step 540 seg_loss 0.13717382  ver_loss 0.19655196  precision 1.00000000  recall 0.00042240  loss1 0.38503969  loss2 0.33372581  loss3 20744.13281250 
val epoch 12 step 12 seg_loss 0.10723348  ver_loss 0.19929068  precision 1.00000000  recall 0.00033760  loss1 0.33993137  loss2 0.30652401  loss3 0.00076904 
train epoch 13 step 585 seg_loss 0.12976897  ver_loss 0.20063427  precision 1.00000000  recall 0.00046539  loss1 0.37966353  loss2 0.33040327  loss3 26516.57421875 
val epoch 13 step 13 seg_loss 0.10373903  ver_loss 0.19917487  precision 1.00000000  recall 0.00033760  loss1 0.33215716  loss2 0.30291390  loss3 0.00134548 
train epoch 14 step 630 seg_loss 0.13171317  ver_loss 0.20417431  precision 1.00000000  recall 0.00029016  loss1 0.37806952  loss2 0.33588746  loss3 25718.82226562 
val epoch 14 step 14 seg_loss 0.09403366  ver_loss 0.19882147  precision 1.00000000  recall 0.00033760  loss1 0.34032819  loss2 0.29285502  loss3 0.00132484 
train epoch 15 step 675 seg_loss 0.11635264  ver_loss 0.19776185  precision 1.00000000  recall 0.00036347  loss1 0.36314934  loss2 0.31411451  loss3 25395.06250000 
val epoch 15 step 15 seg_loss 0.09972886  ver_loss 0.19805259  precision 1.00000000  recall 0.00033760  loss1 0.32385051  loss2 0.29778156  loss3 0.00126423 
train epoch 16 step 720 seg_loss 0.15226460  ver_loss 0.22290307  precision 1.00000000  recall 0.00031799  loss1 0.41597649  loss2 0.37516767  loss3 13984.32421875 
val epoch 16 step 16 seg_loss 0.09001101  ver_loss 0.19902761  precision 1.00000000  recall 0.00033760  loss1 0.32279727  loss2 0.28903854  loss3 0.00101678 
train epoch 17 step 765 seg_loss 0.18491466  ver_loss 0.19672385  precision 1.00000000  recall 0.00035932  loss1 0.42365128  loss2 0.38163847  loss3 7573.54589844 
val epoch 17 step 17 seg_loss 0.08341219  ver_loss 0.19535612  precision 1.00000000  recall 0.00033760  loss1 0.29826254  loss2 0.27876824  loss3 0.00083013 
train epoch 18 step 810 seg_loss 0.10523951  ver_loss 0.20395511  precision 1.00000000  recall 0.00037949  loss1 0.33979189  loss2 0.30919465  loss3 12427.08984375 
val epoch 18 step 18 seg_loss 0.07758286  ver_loss 0.19342022  precision 1.00000000  recall 0.00033760  loss1 0.28120279  loss2 0.27100322  loss3 0.00045731 
train epoch 19 step 855 seg_loss 0.09598185  ver_loss 0.19672081  precision 1.00000000  recall 0.00032097  loss1 0.31327030  loss2 0.29270267  loss3 14674.10156250 
val epoch 19 step 19 seg_loss 0.06923670  ver_loss 0.19424184  precision 1.00000000  recall 0.00033760  loss1 0.27401367  loss2 0.26347858  loss3 0.00070494 
train epoch 20 step 900 seg_loss 0.08911996  ver_loss 0.19343728  precision 1.00000000  recall 0.00033777  loss1 0.29678398  loss2 0.28255725  loss3 13464.21972656 
val epoch 20 step 20 seg_loss 0.06973825  ver_loss 0.19251323  precision 1.00000000  recall 0.00033760  loss1 0.27491370  loss2 0.26225138  loss3 0.00057620 
train epoch 21 step 945 seg_loss 0.12021729  ver_loss 0.19848990  precision 1.00000000  recall 0.00032656  loss1 0.33499134  loss2 0.31870720  loss3 8521.22851562 
val epoch 21 step 21 seg_loss 0.06646379  ver_loss 0.19237767  precision 1.00000000  recall 0.00033760  loss1 0.26692957  loss2 0.25884151  loss3 0.00057335 
train epoch 22 step 990 seg_loss 0.06684501  ver_loss 0.21495830  precision 1.00000000  recall 0.00041509  loss1 0.29152253  loss2 0.28180331  loss3 13773.29101562 
val epoch 22 step 22 seg_loss 0.06359532  ver_loss 0.19294760  precision 1.00000000  recall 0.00033760  loss1 0.26352310  loss2 0.25654292  loss3 0.00056128 
train epoch 23 step 1035 seg_loss 0.08568564  ver_loss 0.18525764  precision 1.00000000  recall 0.00024959  loss1 0.28319946  loss2 0.27094328  loss3 12610.85546875 
val epoch 23 step 23 seg_loss 0.06320365  ver_loss 0.19053155  precision 1.00000000  recall 0.00033760  loss1 0.26121107  loss2 0.25373527  loss3 0.00055468 
train epoch 24 step 1080 seg_loss 0.07451601  ver_loss 0.20887583  precision 1.00000000  recall 0.00051159  loss1 0.29679015  loss2 0.28339183  loss3 7502.11181641 
val epoch 24 step 24 seg_loss 0.06692000  ver_loss 0.19012205  precision 1.00000000  recall 0.00033760  loss1 0.26321054  loss2 0.25704217  loss3 0.00035906 
train epoch 25 step 1125 seg_loss 0.07816498  ver_loss 0.20533478  precision 1.00000000  recall 0.00036630  loss1 0.29374951  loss2 0.28349978  loss3 8372.81835938 
val epoch 25 step 25 seg_loss 0.05826350  ver_loss 0.19462532  precision 1.00000000  recall 0.00033760  loss1 0.25523037  loss2 0.25288880  loss3 0.00053524 
train epoch 26 step 1170 seg_loss 0.09544467  ver_loss 0.20242915  precision 1.00000000  recall 0.00046644  loss1 0.30913872  loss2 0.29787382  loss3 4453.02929688 
val epoch 26 step 26 seg_loss 0.06023014  ver_loss 0.19248328  precision 1.00000000  recall 0.00033760  loss1 0.25380218  loss2 0.25271329  loss3 0.00052100 
train epoch 27 step 1215 seg_loss 0.08056909  ver_loss 0.20048730  precision 1.00000000  recall 0.00053427  loss1 0.28698060  loss2 0.28105640  loss3 6116.85302734 
val epoch 27 step 27 seg_loss 0.06184459  ver_loss 0.18822651  precision 1.00000000  recall 0.00033760  loss1 0.25429204  loss2 0.25007111  loss3 0.00042892 
train epoch 28 step 1260 seg_loss 0.18759762  ver_loss 0.17896403  precision 1.00000000  recall 0.00037698  loss1 0.36554945  loss2 0.36656165  loss3 3986.12817383 
val epoch 28 step 28 seg_loss 0.05959602  ver_loss 0.19207160  precision 1.00000000  recall 0.00033760  loss1 0.25878718  loss2 0.25166768  loss3 0.00047857 
train epoch 29 step 1305 seg_loss 0.07597607  ver_loss 0.18648282  precision 1.00000000  recall 0.00034324  loss1 0.27081525  loss2 0.26245889  loss3 7694.21386719 
val epoch 29 step 29 seg_loss 0.06863673  ver_loss 0.18750082  precision 1.00000000  recall 0.00033760  loss1 0.26094368  loss2 0.25613755  loss3 0.00024907 
train epoch 30 step 1350 seg_loss 0.07257937  ver_loss 0.19178735  precision 1.00000000  recall 0.00027415  loss1 0.27041793  loss2 0.26436669  loss3 9432.73925781 
val epoch 30 step 30 seg_loss 0.07253686  ver_loss 0.18503878  precision 1.00000000  recall 0.00033760  loss1 0.26230991  loss2 0.25757554  loss3 0.00025961 
train epoch 31 step 1395 seg_loss 0.11146303  ver_loss 0.18663061  precision 1.00000000  recall 0.00042424  loss1 0.30742559  loss2 0.29809362  loss3 3211.56176758 
val epoch 31 step 31 seg_loss 0.06848538  ver_loss 0.18238825  precision 1.00000000  recall 0.00033760  loss1 0.25492975  loss2 0.25087368  loss3 0.00027855 
train epoch 32 step 1440 seg_loss 0.10496984  ver_loss 0.19756092  precision 1.00000000  recall 0.00036103  loss1 0.30765218  loss2 0.30253077  loss3 3415.20703125 
val epoch 32 step 32 seg_loss 0.06219286  ver_loss 0.18418854  precision 1.00000000  recall 0.00033760  loss1 0.25150982  loss2 0.24638133  loss3 0.00034301 
train epoch 33 step 1485 seg_loss 0.05274675  ver_loss 0.20461982  precision 1.00000000  recall 0.00109413  loss1 0.26298001  loss2 0.25736657  loss3 4568.76367188 
val epoch 33 step 33 seg_loss 0.06851143  ver_loss 0.18242368  precision 1.00000000  recall 0.00033760  loss1 0.25380450  loss2 0.25093517  loss3 0.00025421 
train epoch 34 step 1530 seg_loss 0.11505259  ver_loss 0.19168034  precision 1.00000000  recall 0.00032318  loss1 0.31071889  loss2 0.30673292  loss3 3236.38769531 
val epoch 34 step 34 seg_loss 0.06468941  ver_loss 0.18223265  precision 1.00000000  recall 0.00033760  loss1 0.25186139  loss2 0.24692221  loss3 0.00028814 
train epoch 35 step 1575 seg_loss 0.08317113  ver_loss 0.18916480  precision 1.00000000  recall 0.00041939  loss1 0.27418345  loss2 0.27233595  loss3 3584.72143555 
val epoch 35 step 35 seg_loss 0.06829175  ver_loss 0.18099257  precision 1.00000000  recall 0.00033760  loss1 0.25272274  loss2 0.24928421  loss3 0.00020212 
train epoch 36 step 1620 seg_loss 0.05446722  ver_loss 0.20060346  precision 1.00000000  recall 0.00044258  loss1 0.25759041  loss2 0.25507069  loss3 7314.67675781 
val epoch 36 step 36 seg_loss 0.06502284  ver_loss 0.18061601  precision 1.00000000  recall 0.00033760  loss1 0.24894126  loss2 0.24563888  loss3 0.00022261 
train epoch 37 step 1665 seg_loss 0.05668789  ver_loss 0.16498867  precision 1.00000000  recall 0.00044547  loss1 0.22503659  loss2 0.22167656  loss3 6661.44384766 
val epoch 37 step 37 seg_loss 0.06310674  ver_loss 0.18159585  precision 1.00000000  recall 0.00033760  loss1 0.24531917  loss2 0.24470232  loss3 0.00026040 
train epoch 38 step 1710 seg_loss 0.17825222  ver_loss 0.19562319  precision 1.00000000  recall 0.00051494  loss1 0.37540719  loss2 0.37387541  loss3 685.14050293 
val epoch 38 step 38 seg_loss 0.06349664  ver_loss 0.18607484  precision 1.00000000  recall 0.00033760  loss1 0.24723879  loss2 0.24957147  loss3 0.00029372 
train epoch 39 step 1755 seg_loss 0.10929517  ver_loss 0.22260526  precision 1.00000000  recall 0.00035278  loss1 0.33305994  loss2 0.33190042  loss3 2267.08789062 
val epoch 39 step 39 seg_loss 0.06578828  ver_loss 0.18386756  precision 1.00000000  recall 0.00033760  loss1 0.25218156  loss2 0.24965592  loss3 0.00020530 
train epoch 40 step 1800 seg_loss 0.07531108  ver_loss 0.19126922  precision 1.00000000  recall 0.00031222  loss1 0.26773176  loss2 0.26658028  loss3 4399.04589844 
val epoch 40 step 40 seg_loss 0.06979842  ver_loss 0.18138266  precision 1.00000000  recall 0.00033760  loss1 0.25419980  loss2 0.25118122  loss3 0.00013814 
train epoch 41 step 1845 seg_loss 0.04866766  ver_loss 0.17062458  precision 1.00000000  recall 0.00050347  loss1 0.22123115  loss2 0.21929225  loss3 7006.85400391 
val epoch 41 step 41 seg_loss 0.07378858  ver_loss 0.18031228  precision 1.00000000  recall 0.00033760  loss1 0.25633368  loss2 0.25410083  loss3 0.00011085 
train epoch 42 step 1890 seg_loss 0.16145372  ver_loss 0.19911440  precision 1.00000000  recall 0.00035320  loss1 0.36008614  loss2 0.36056811  loss3 736.63269043 
val epoch 42 step 42 seg_loss 0.06436221  ver_loss 0.18350215  precision 1.00000000  recall 0.00033760  loss1 0.25034958  loss2 0.24786435  loss3 0.00021706 
train epoch 43 step 1935 seg_loss 0.07607195  ver_loss 0.20157765  precision 1.00000000  recall 0.00032494  loss1 0.27868831  loss2 0.27764961  loss3 4197.44384766 
val epoch 43 step 43 seg_loss 0.06562094  ver_loss 0.18336114  precision 1.00000000  recall 0.00033760  loss1 0.25154984  loss2 0.24898212  loss3 0.00018116 
train epoch 44 step 1980 seg_loss 0.12341656  ver_loss 0.19243836  precision 1.00000000  recall 0.00035707  loss1 0.31548625  loss2 0.31585494  loss3 1235.14282227 
val epoch 44 step 44 seg_loss 0.06624764  ver_loss 0.18567353  precision 1.00000000  recall 0.00033760  loss1 0.25508174  loss2 0.25192121  loss3 0.00015817 
train epoch 45 step 2025 seg_loss 0.17657611  ver_loss 0.19544287  precision 1.00000000  recall 0.00025764  loss1 0.37164629  loss2 0.37201899  loss3 1137.01684570 
val epoch 45 step 45 seg_loss 0.06540145  ver_loss 0.18720394  precision 1.00000000  recall 0.00033760  loss1 0.25455976  loss2 0.25260544  loss3 0.00018430 
train epoch 46 step 2070 seg_loss 0.14000928  ver_loss 0.20154916  precision 1.00000000  recall 0.00030081  loss1 0.34142396  loss2 0.34155846  loss3 959.06909180 
val epoch 46 step 46 seg_loss 0.07418348  ver_loss 0.19139616  precision 1.00000000  recall 0.00033760  loss1 0.26498464  loss2 0.26557970  loss3 0.00008722 
train epoch 47 step 2115 seg_loss 0.15275261  ver_loss 0.20731160  precision 1.00000000  recall 0.00033792  loss1 0.36017764  loss2 0.36006421  loss3 1063.24267578 
val epoch 47 step 47 seg_loss 0.06813462  ver_loss 0.19027701  precision 1.00000000  recall 0.00033760  loss1 0.25959286  loss2 0.25841159  loss3 0.00012902 
train epoch 48 step 2160 seg_loss 0.09081590  ver_loss 0.20525455  precision 1.00000000  recall 0.00084874  loss1 0.29579949  loss2 0.29607046  loss3 643.01135254 
val epoch 48 step 48 seg_loss 0.06776138  ver_loss 0.18993846  precision 1.00000000  recall 0.00033760  loss1 0.25838533  loss2 0.25769979  loss3 0.00012356 
train epoch 49 step 2205 seg_loss 0.10801255  ver_loss 0.20605943  precision 1.00000000  recall 0.00036458  loss1 0.31343317  loss2 0.31407195  loss3 1537.14501953 
val epoch 49 step 49 seg_loss 0.06499074  ver_loss 0.18771049  precision 1.00000000  recall 0.00033760  loss1 0.25359470  loss2 0.25270101  loss3 0.00016774 
train epoch 50 step 2250 seg_loss 0.05634797  ver_loss 0.19139537  precision 1.00000000  recall 0.00044048  loss1 0.24802414  loss2 0.24774334  loss3 4568.26074219 
val epoch 50 step 50 seg_loss 0.06842063  ver_loss 0.18888901  precision 1.00000000  recall 0.00033760  loss1 0.25892204  loss2 0.25730956  loss3 0.00014470 
train epoch 51 step 2295 seg_loss 0.17815939  ver_loss 0.19809085  precision 1.00000000  recall 0.00038502  loss1 0.37338915  loss2 0.37625024  loss3 583.31115723 
val epoch 51 step 51 seg_loss 0.07342479  ver_loss 0.18655752  precision 1.00000000  recall 0.00033760  loss1 0.25958657  loss2 0.25998229  loss3 0.00008461 
train epoch 52 step 2340 seg_loss 0.15553632  ver_loss 0.18576516  precision 1.00000000  recall 0.00038589  loss1 0.33961368  loss2 0.34130147  loss3 767.85638428 
val epoch 52 step 52 seg_loss 0.07135337  ver_loss 0.18997529  precision 1.00000000  recall 0.00033760  loss1 0.26185790  loss2 0.26132882  loss3 0.00010213 
train epoch 53 step 2385 seg_loss 0.17331594  ver_loss 0.20054916  precision 1.00000000  recall 0.00040982  loss1 0.37251741  loss2 0.37386513  loss3 313.68823242 
val epoch 53 step 53 seg_loss 0.07835484  ver_loss 0.18788067  precision 1.00000000  recall 0.00033760  loss1 0.26536438  loss2 0.26623553  loss3 0.00006029 
train epoch 54 step 2430 seg_loss 0.10925800  ver_loss 0.18575116  precision 1.00000000  recall 0.00028329  loss1 0.29520047  loss2 0.29500914  loss3 1619.56176758 
val epoch 54 step 54 seg_loss 0.07739127  ver_loss 0.19213104  precision 1.00000000  recall 0.00033760  loss1 0.26883426  loss2 0.26952240  loss3 0.00005727 
train epoch 55 step 2475 seg_loss 0.10557508  ver_loss 0.22547269  precision 1.00000000  recall 0.00035468  loss1 0.33020478  loss2 0.33104777  loss3 1022.36315918 
val epoch 55 step 55 seg_loss 0.07591016  ver_loss 0.19130459  precision 1.00000000  recall 0.00033760  loss1 0.26661152  loss2 0.26721489  loss3 0.00005677 
train epoch 56 step 2520 seg_loss 0.07025542  ver_loss 0.20499866  precision 1.00000000  recall 0.00076635  loss1 0.27482897  loss2 0.27525407  loss3 1432.12646484 
val epoch 56 step 56 seg_loss 0.07775425  ver_loss 0.19182429  precision 1.00000000  recall 0.00033760  loss1 0.26923066  loss2 0.26957861  loss3 0.00004776 
train epoch 57 step 2565 seg_loss 0.10561088  ver_loss 0.20839447  precision 1.00000000  recall 0.00027817  loss1 0.31350067  loss2 0.31400535  loss3 1790.35058594 
val epoch 57 step 57 seg_loss 0.07464587  ver_loss 0.19057542  precision 1.00000000  recall 0.00033760  loss1 0.26509067  loss2 0.26522136  loss3 0.00005344 
train epoch 58 step 2610 seg_loss 0.08168410  ver_loss 0.21090056  precision 1.00000000  recall 0.00041949  loss1 0.29260939  loss2 0.29258466  loss3 1138.40722656 
val epoch 58 step 58 seg_loss 0.07495172  ver_loss 0.19166966  precision 1.00000000  recall 0.00033760  loss1 0.26789972  loss2 0.26662150  loss3 0.00006039 
train epoch 59 step 2655 seg_loss 0.07727917  ver_loss 0.21561453  precision 1.00000000  recall 0.00040580  loss1 0.29298636  loss2 0.29289368  loss3 1312.12475586 
val epoch 59 step 59 seg_loss 0.07925734  ver_loss 0.19120435  precision 1.00000000  recall 0.00033760  loss1 0.26921812  loss2 0.27046174  loss3 0.00004618 
train epoch 60 step 2700 seg_loss 0.08853438  ver_loss 0.19556636  precision 1.00000000  recall 0.00034570  loss1 0.28440017  loss2 0.28410074  loss3 1480.72973633 
val epoch 60 step 60 seg_loss 0.07704017  ver_loss 0.18922649  precision 1.00000000  recall 0.00033760  loss1 0.26615897  loss2 0.26626670  loss3 0.00004381 
train epoch 61 step 2745 seg_loss 0.16115673  ver_loss 0.20493928  precision 1.00000000  recall 0.00032139  loss1 0.36436111  loss2 0.36609602  loss3 289.88446045 
val epoch 61 step 61 seg_loss 0.07305450  ver_loss 0.19536886  precision 1.00000000  recall 0.00033760  loss1 0.26677313  loss2 0.26842347  loss3 0.00007102 
train epoch 62 step 2790 seg_loss 0.10933215  ver_loss 0.20161203  precision 1.00000000  recall 0.00031928  loss1 0.31055591  loss2 0.31094420  loss3 622.04736328 
val epoch 62 step 62 seg_loss 0.07424769  ver_loss 0.19448008  precision 1.00000000  recall 0.00033760  loss1 0.26675788  loss2 0.26872763  loss3 0.00005907 
train epoch 63 step 2835 seg_loss 0.11427172  ver_loss 0.19551574  precision 1.00000000  recall 0.00046275  loss1 0.30931297  loss2 0.30978745  loss3 313.96734619 
val epoch 63 step 63 seg_loss 0.07583710  ver_loss 0.19586939  precision 1.00000000  recall 0.00033760  loss1 0.27005413  loss2 0.27170622  loss3 0.00004423 
train epoch 64 step 2880 seg_loss 0.13260254  ver_loss 0.20219801  precision 1.00000000  recall 0.00040229  loss1 0.33315665  loss2 0.33480054  loss3 253.06289673 
val epoch 64 step 64 seg_loss 0.07660549  ver_loss 0.19477102  precision 1.00000000  recall 0.00033760  loss1 0.26963142  loss2 0.27137640  loss3 0.00004174 
train epoch 65 step 2925 seg_loss 0.14376760  ver_loss 0.22607613  precision 1.00000000  recall 0.00033678  loss1 0.36824471  loss2 0.36984372  loss3 360.31317139 
val epoch 65 step 65 seg_loss 0.07302793  ver_loss 0.19532272  precision 1.00000000  recall 0.00033760  loss1 0.26749665  loss2 0.26835057  loss3 0.00005621 
train epoch 66 step 2970 seg_loss 0.09312214  ver_loss 0.20377433  precision 1.00000000  recall 0.00029053  loss1 0.29671714  loss2 0.29689646  loss3 1355.06921387 
val epoch 66 step 66 seg_loss 0.07726946  ver_loss 0.19702546  precision 1.00000000  recall 0.00033760  loss1 0.27253845  loss2 0.27429503  loss3 0.00003628 
train epoch 67 step 3015 seg_loss 0.09911841  ver_loss 0.20441048  precision 1.00000000  recall 0.00032339  loss1 0.30266958  loss2 0.30352890  loss3 955.55993652 
val epoch 67 step 67 seg_loss 0.07654221  ver_loss 0.19322897  precision 1.00000000  recall 0.00033760  loss1 0.26760662  loss2 0.26977128  loss3 0.00003850 
train epoch 68 step 3060 seg_loss 0.13057938  ver_loss 0.20643562  precision 1.00000000  recall 0.00029663  loss1 0.33581147  loss2 0.33701500  loss3 463.31210327 
val epoch 68 step 68 seg_loss 0.07756137  ver_loss 0.19388288  precision 1.00000000  recall 0.00033760  loss1 0.26956969  loss2 0.27144423  loss3 0.00003347 
train epoch 69 step 3105 seg_loss 0.07512619  ver_loss 0.20522371  precision 1.00000000  recall 0.00038440  loss1 0.27954125  loss2 0.28034991  loss3 1375.89453125 
val epoch 69 step 69 seg_loss 0.07512917  ver_loss 0.19380511  precision 1.00000000  recall 0.00033760  loss1 0.26782444  loss2 0.26893413  loss3 0.00003751 
train epoch 70 step 3150 seg_loss 0.09049808  ver_loss 0.19649149  precision 1.00000000  recall 0.00049530  loss1 0.28586057  loss2 0.28698957  loss3 371.05987549 
val epoch 70 step 70 seg_loss 0.07728540  ver_loss 0.19545192  precision 1.00000000  recall 0.00033760  loss1 0.27116504  loss2 0.27273744  loss3 0.00003045 
train epoch 71 step 3195 seg_loss 0.07408024  ver_loss 0.18688807  precision 1.00000000  recall 0.00049674  loss1 0.26058275  loss2 0.26096833  loss3 924.78063965 
val epoch 71 step 71 seg_loss 0.07545872  ver_loss 0.19291960  precision 1.00000000  recall 0.00033760  loss1 0.26659775  loss2 0.26837838  loss3 0.00003465 
train epoch 72 step 3240 seg_loss 0.07725919  ver_loss 0.18757409  precision 1.00000000  recall 0.00040740  loss1 0.26444614  loss2 0.26483330  loss3 966.74017334 
val epoch 72 step 72 seg_loss 0.07756484  ver_loss 0.19683731  precision 1.00000000  recall 0.00033760  loss1 0.27298087  loss2 0.27440211  loss3 0.00002588 
train epoch 73 step 3285 seg_loss 0.11661814  ver_loss 0.18017063  precision 1.00000000  recall 0.00034664  loss1 0.29528749  loss2 0.29678878  loss3 255.09658813 
val epoch 73 step 73 seg_loss 0.07399008  ver_loss 0.19499792  precision 1.00000000  recall 0.00033760  loss1 0.26803127  loss2 0.26898789  loss3 0.00003551 
train epoch 74 step 3330 seg_loss 0.16597250  ver_loss 0.20576927  precision 1.00000000  recall 0.00028490  loss1 0.36909825  loss2 0.37174177  loss3 181.16259766 
val epoch 74 step 74 seg_loss 0.07587182  ver_loss 0.19415665  precision 1.00000000  recall 0.00033760  loss1 0.26843509  loss2 0.27002841  loss3 0.00002804 
train epoch 75 step 3375 seg_loss 0.08808663  ver_loss 0.20300737  precision 1.00000000  recall 0.00038245  loss1 0.29064220  loss2 0.29109401  loss3 787.65637207 
val epoch 75 step 75 seg_loss 0.07572647  ver_loss 0.19553649  precision 1.00000000  recall 0.00033760  loss1 0.26992771  loss2 0.27126271  loss3 0.00002556 
train epoch 76 step 3420 seg_loss 0.11672953  ver_loss 0.18968368  precision 1.00000000  recall 0.00028410  loss1 0.30608436  loss2 0.30641317  loss3 324.50170898 
val epoch 76 step 76 seg_loss 0.07919911  ver_loss 0.19727339  precision 1.00000000  recall 0.00033760  loss1 0.27392685  loss2 0.27647248  loss3 0.00002830 
train epoch 77 step 3465 seg_loss 0.08415299  ver_loss 0.19485387  precision 1.00000000  recall 0.00045337  loss1 0.27833959  loss2 0.27900690  loss3 581.78552246 
val epoch 77 step 77 seg_loss 0.07377138  ver_loss 0.19436951  precision 1.00000000  recall 0.00033760  loss1 0.26659885  loss2 0.26814073  loss3 0.00002930 
train epoch 78 step 3510 seg_loss 0.07843158  ver_loss 0.18558323  precision 1.00000000  recall 0.00037340  loss1 0.26350021  loss2 0.26401481  loss3 1023.40014648 
val epoch 78 step 78 seg_loss 0.07620665  ver_loss 0.19555715  precision 1.00000000  recall 0.00033760  loss1 0.27042720  loss2 0.27176371  loss3 0.00002113 
train epoch 79 step 3555 seg_loss 0.11817189  ver_loss 0.19944203  precision 1.00000000  recall 0.00031165  loss1 0.31652951  loss2 0.31761390  loss3 373.39111328 
val epoch 79 step 79 seg_loss 0.07426338  ver_loss 0.19584334  precision 1.00000000  recall 0.00033760  loss1 0.26912490  loss2 0.27010664  loss3 0.00002456 
train epoch 80 step 3600 seg_loss 0.10643201  ver_loss 0.20264423  precision 1.00000000  recall 0.00033109  loss1 0.30823821  loss2 0.30907625  loss3 217.60580444 
val epoch 80 step 80 seg_loss 0.07473331  ver_loss 0.19630370  precision 1.00000000  recall 0.00033760  loss1 0.26990795  loss2 0.27103707  loss3 0.00002226 
train epoch 81 step 3645 seg_loss 0.14126804  ver_loss 0.22226703  precision 1.00000000  recall 0.00034509  loss1 0.36220232  loss2 0.36353505  loss3 178.72830200 
val epoch 81 step 81 seg_loss 0.07280844  ver_loss 0.19467640  precision 1.00000000  recall 0.00033760  loss1 0.26659411  loss2 0.26748484  loss3 0.00003074 
train epoch 82 step 3690 seg_loss 0.09066729  ver_loss 0.19347295  precision 1.00000000  recall 0.00032227  loss1 0.28314680  loss2 0.28414026  loss3 649.43933105 
val epoch 82 step 82 seg_loss 0.07236987  ver_loss 0.19552170  precision 1.00000000  recall 0.00033760  loss1 0.26703340  loss2 0.26789159  loss3 0.00003455 
train epoch 83 step 3735 seg_loss 0.15208258  ver_loss 0.18894950  precision 1.00000000  recall 0.00032873  loss1 0.34014812  loss2 0.34103206  loss3 144.94750977 
val epoch 83 step 83 seg_loss 0.07560514  ver_loss 0.19571240  precision 1.00000000  recall 0.00033760  loss1 0.26988542  loss2 0.27131748  loss3 0.00001928 
train epoch 84 step 3780 seg_loss 0.07385997  ver_loss 0.21097155  precision 1.00000000  recall 0.00046800  loss1 0.28367797  loss2 0.28483152  loss3 623.65417480 
val epoch 84 step 84 seg_loss 0.07353474  ver_loss 0.19549011  precision 1.00000000  recall 0.00033760  loss1 0.26830551  loss2 0.26902470  loss3 0.00002522 
train epoch 85 step 3825 seg_loss 0.08304901  ver_loss 0.18460259  precision 1.00000000  recall 0.00048589  loss1 0.26714179  loss2 0.26765162  loss3 437.79693604 
val epoch 85 step 85 seg_loss 0.07363319  ver_loss 0.19576134  precision 1.00000000  recall 0.00033760  loss1 0.26839244  loss2 0.26939452  loss3 0.00002713 
train epoch 86 step 3870 seg_loss 0.11509139  ver_loss 0.19125840  precision 1.00000000  recall 0.00043895  loss1 0.30536348  loss2 0.30634978  loss3 146.85453796 
val epoch 86 step 86 seg_loss 0.07708791  ver_loss 0.19718535  precision 1.00000000  recall 0.00033760  loss1 0.27279878  loss2 0.27427337  loss3 0.00001846 
train epoch 87 step 3915 seg_loss 0.11543251  ver_loss 0.21223986  precision 1.00000000  recall 0.00033999  loss1 0.32673955  loss2 0.32767239  loss3 315.49118042 
val epoch 87 step 87 seg_loss 0.07741366  ver_loss 0.19639660  precision 1.00000000  recall 0.00033760  loss1 0.27196875  loss2 0.27381036  loss3 0.00001978 
train epoch 88 step 3960 seg_loss 0.19104065  ver_loss 0.20579056  precision 1.00000000  recall 0.00027577  loss1 0.39578384  loss2 0.39683121  loss3 130.12358093 
val epoch 88 step 88 seg_loss 0.07728612  ver_loss 0.19547597  precision 1.00000000  recall 0.00033760  loss1 0.27093232  loss2 0.27276194  loss3 0.00001987 
train epoch 89 step 4005 seg_loss 0.15477487  ver_loss 0.23355988  precision 1.00000000  recall 0.00048884  loss1 0.38413814  loss2 0.38833475  loss3 128.40295410 
val epoch 89 step 89 seg_loss 0.07829937  ver_loss 0.19516358  precision 1.00000000  recall 0.00033760  loss1 0.27130905  loss2 0.27346304  loss3 0.00002078 
train epoch 90 step 4050 seg_loss 0.12019657  ver_loss 0.24546188  precision 1.00000000  recall 0.00042276  loss1 0.36393523  loss2 0.36565843  loss3 138.99227905 
val epoch 90 step 90 seg_loss 0.07737355  ver_loss 0.19588244  precision 1.00000000  recall 0.00033760  loss1 0.27178419  loss2 0.27325591  loss3 0.00001782 
train epoch 91 step 4095 seg_loss 0.09019265  ver_loss 0.20235965  precision 1.00000000  recall 0.00039277  loss1 0.29168326  loss2 0.29255229  loss3 168.07775879 
val epoch 91 step 91 seg_loss 0.07654037  ver_loss 0.19436708  precision 1.00000000  recall 0.00033760  loss1 0.26947922  loss2 0.27090743  loss3 0.00001818 
train epoch 92 step 4140 seg_loss 0.09524048  ver_loss 0.19296159  precision 1.00000000  recall 0.00047388  loss1 0.28737172  loss2 0.28820205  loss3 155.75355530 
val epoch 92 step 92 seg_loss 0.07587641  ver_loss 0.19522147  precision 1.00000000  recall 0.00033760  loss1 0.26991895  loss2 0.27109793  loss3 0.00001914 
train epoch 93 step 4185 seg_loss 0.10012282  ver_loss 0.19182166  precision 1.00000000  recall 0.00033703  loss1 0.29098505  loss2 0.29194447  loss3 334.36364746 
train epoch 93 step 4185 seg_loss 0.09474958  ver_loss 0.20204723  precision 1.00000000  recall 0.00032125  loss1 0.29635513  loss2 0.29679680  loss3 335.56585693 
val epoch 93 step 93 seg_loss 0.07797194  ver_loss 0.19571240  precision 1.00000000  recall 0.00033760  loss1 0.27135274  loss2 0.27368408  loss3 0.00002150 
train epoch 94 step 4230 seg_loss 0.11180286  ver_loss 0.21612105  precision 1.00000000  recall 0.00028240  loss1 0.32714307  loss2 0.32792389  loss3 236.67597961 
val epoch 94 step 94 seg_loss 0.07626847  ver_loss 0.19579226  precision 1.00000000  recall 0.00033760  loss1 0.27076215  loss2 0.27206078  loss3 0.00001840 
train epoch 95 step 4275 seg_loss 0.09186953  ver_loss 0.20469519  precision 1.00000000  recall 0.00038936  loss1 0.29478419  loss2 0.29656470  loss3 206.48689270 
val epoch 95 step 95 seg_loss 0.07705694  ver_loss 0.19664009  precision 1.00000000  recall 0.00033760  loss1 0.27260754  loss2 0.27369705  loss3 0.00001815 
train epoch 96 step 4320 seg_loss 0.17661628  ver_loss 0.19839706  precision 1.00000000  recall 0.00035438  loss1 0.37123138  loss2 0.37501335  loss3 103.79832458 
val epoch 96 step 96 seg_loss 0.07844039  ver_loss 0.19608656  precision 1.00000000  recall 0.00033760  loss1 0.27294111  loss2 0.27452710  loss3 0.00001764 
train epoch 97 step 4365 seg_loss 0.10246743  ver_loss 0.19849084  precision 1.00000000  recall 0.00065971  loss1 0.30006173  loss2 0.30095828  loss3 142.45997620 
val epoch 97 step 97 seg_loss 0.07938293  ver_loss 0.19789088  precision 1.00000000  recall 0.00033760  loss1 0.27535072  loss2 0.27727380  loss3 0.00001989 
train epoch 98 step 4410 seg_loss 0.09779982  ver_loss 0.20956230  precision 1.00000000  recall 0.00037804  loss1 0.30632681  loss2 0.30736214  loss3 131.33859253 
val epoch 98 step 98 seg_loss 0.07886634  ver_loss 0.19702111  precision 1.00000000  recall 0.00033760  loss1 0.27432430  loss2 0.27588755  loss3 0.00001555 
train epoch 99 step 4455 seg_loss 0.10470291  ver_loss 0.20062098  precision 1.00000000  recall 0.00035247  loss1 0.30342251  loss2 0.30532390  loss3 136.70678711 
val epoch 99 step 99 seg_loss 0.07881514  ver_loss 0.19637163  precision 1.00000000  recall 0.00033760  loss1 0.27349481  loss2 0.27518678  loss3 0.00001676 
train epoch 100 step 4500 seg_loss 0.14011739  ver_loss 0.18098050  precision 1.00000000  recall 0.00036719  loss1 0.31908298  loss2 0.32109788  loss3 130.57299805 
val epoch 100 step 100 seg_loss 0.07873967  ver_loss 0.19671638  precision 1.00000000  recall 0.00033760  loss1 0.27374700  loss2 0.27545595  loss3 0.00001986 
train epoch 101 step 4545 seg_loss 0.09381899  ver_loss 0.19655797  precision 1.00000000  recall 0.00038130  loss1 0.29000360  loss2 0.29037696  loss3 128.11296082 
val epoch 101 step 101 seg_loss 0.07821709  ver_loss 0.19620301  precision 1.00000000  recall 0.00033760  loss1 0.27302459  loss2 0.27442002  loss3 0.00001476 
train epoch 102 step 4590 seg_loss 0.12096151  ver_loss 0.22516118  precision 1.00000000  recall 0.00047149  loss1 0.34532464  loss2 0.34612268  loss3 91.37341309 
val epoch 102 step 102 seg_loss 0.07813743  ver_loss 0.19599822  precision 1.00000000  recall 0.00033760  loss1 0.27291536  loss2 0.27413544  loss3 0.00001432 
train epoch 103 step 4635 seg_loss 0.09502479  ver_loss 0.18170640  precision 1.00000000  recall 0.00037649  loss1 0.27590480  loss2 0.27673119  loss3 151.53491211 
val epoch 103 step 103 seg_loss 0.07807717  ver_loss 0.19549398  precision 1.00000000  recall 0.00033760  loss1 0.27189061  loss2 0.27357101  loss3 0.00001542 
train epoch 104 step 4680 seg_loss 0.09818867  ver_loss 0.20629948  precision 1.00000000  recall 0.00038433  loss1 0.30315655  loss2 0.30448818  loss3 152.86402893 
val epoch 104 step 104 seg_loss 0.07677346  ver_loss 0.19589268  precision 1.00000000  recall 0.00033760  loss1 0.27161196  loss2 0.27266634  loss3 0.00001325 
train epoch 105 step 4725 seg_loss 0.09343992  ver_loss 0.22470881  precision 1.00000000  recall 0.00031353  loss1 0.31631410  loss2 0.31814873  loss3 218.32235718 
val epoch 105 step 105 seg_loss 0.07753134  ver_loss 0.19645227  precision 1.00000000  recall 0.00033760  loss1 0.27264950  loss2 0.27398348  loss3 0.00001327 
train epoch 106 step 4770 seg_loss 0.11206937  ver_loss 0.20378560  precision 1.00000000  recall 0.00038431  loss1 0.31512374  loss2 0.31585494  loss3 104.45848846 
val epoch 106 step 106 seg_loss 0.07782722  ver_loss 0.19621952  precision 1.00000000  recall 0.00033760  loss1 0.27259460  loss2 0.27404699  loss3 0.00001320 
train epoch 107 step 4815 seg_loss 0.13047412  ver_loss 0.18784045  precision 1.00000000  recall 0.00034600  loss1 0.31636053  loss2 0.31831458  loss3 113.22912598 
